---
title: 'Production LLMs & AI Agents on OCI: Part 3 - The Operating Model'
date: '2026-01-21'
lastUpdated: '2026-01-21'
description: 'The definitive guide to operating production AI systems. 5-stage maturity model, evaluation pipelines, MLOps for GenAI, incident response runbooks, cost optimization, and team operating models.'
image: '/images/blog/oci-operating-model-hero.jpg'
tags: ['Enterprise AI', 'OCI', 'Governance', 'Observability', 'MLOps', 'Oracle', 'Operating Model']
author: "Frank"
category: 'Enterprise AI'
readingTime: '25 min read'
series: 'Production LLMs & AI Agents on OCI'
seriesPart: 3
---

# Production LLMs and AI Agents on OCI: The Operating Model

**TL;DR:** Production GenAI systems fail not because of bad models, but because of missing operating models. This guide provides a complete 5-stage maturity framework, evaluation pipelines, MLOps architecture, incident runbooks, cost optimization strategies, and team operating models—all mapped to OCI services.

---

## The Operating Model Imperative

Most GenAI projects invest 90% effort on models and 10% on operations. Production systems require the inverse: **the model is a dependency; the operating model is the product.**

<ArchitectureImage
  src="/images/blog/oci-series/operating-model-inversion.png"
  alt="The Operating Model Inversion — POC allocates 90% effort to model/prompt and 10% to operations. Production inverts this: Governance 15%, Observability 20%, Evaluation 15%, Cost Management 10%, Security 15%, Model/Prompt 25%."
  caption="POC effort is 90% model-centric. Production effort distributes across governance, observability, evaluation, cost, security, and the model itself."
/>

An operating model answers these critical questions:

| Question | Why It Matters | Consequence of Not Answering |
|----------|----------------|------------------------------|
| How do we version prompt changes? | Reproducibility, rollback capability | Cannot diagnose regressions |
| How do we know if quality is degrading? | Early detection of model drift | Users discover issues before you |
| How do we control costs? | Budget predictability | Surprise bills, service shutdown |
| How do we respond to incidents? | Mean Time To Recovery | Extended outages, reputation damage |
| How do we maintain compliance? | Regulatory requirements | Fines, audit failures |

---

## The 5-Stage AI Operations Maturity Model

Based on production learnings from enterprise deployments and aligned with industry frameworks like Google's ADK maturity guidance.

<ArchitectureImage
  src="/images/blog/oci-series/maturity-model-5-stages.png"
  alt="5-Stage AI Operations Maturity — Stage 1 Ad-Hoc (heroic firefighting), Stage 2 Managed (basic monitoring), Stage 3 Measured (full observability and SLOs), Stage 4 Optimized (A/B testing and quality gates), Stage 5 Autonomous (self-healing, AI monitors AI)."
  caption="Progress from ad-hoc firefighting to autonomous self-healing operations. Most enterprise teams are between Stage 1 and Stage 2."
/>

### Maturity Assessment Scorecard

Rate your current state (1-5) for each dimension:

| Dimension | Stage 1 | Stage 2 | Stage 3 | Stage 4 | Stage 5 |
|-----------|---------|---------|---------|---------|---------|
| **Versioning** | None | Git for prompts | Semantic versioning + registry | Automated promotion | ML-selected versions |
| **Monitoring** | None | Error logs only | Full traces + metrics | Anomaly detection | Predictive alerts |
| **Evaluation** | Manual testing | Golden sets | Automated in CI/CD | Online A/B tests | Continuous learning |
| **Cost Mgmt** | None | Bill review | Per-request tracking | Budget enforcement | Dynamic optimization |
| **Incident Response** | Ad-hoc | Runbooks | Automated detection | Auto-remediation | Predictive prevention |
| **Governance** | None | Manual review | Policy-as-code | Automated compliance | Continuous audit |

**Scoring:**
- 6-12: Stage 1 (Ad-hoc)
- 13-18: Stage 2 (Managed)
- 19-24: Stage 3 (Measured)
- 25-28: Stage 4 (Optimized)
- 29-30: Stage 5 (Autonomous)

---

## Stage-by-Stage Implementation Roadmap

### Stage 1 → Stage 2: Foundation (Weeks 1-4)

<DecisionBox title="Stage 2 Implementation (Weeks 1-4)">

**Week 1 — Version Control:** Move prompts to Git, define naming conventions, create prompt registry YAML.

**Week 2 — Basic Monitoring:** Enable OCI Logging for all requests, create error dashboards, set up basic alerting (errors, latency).

**Week 3 — Incident Response:** Document runbooks for common failures, define severity levels, establish on-call rotation.

**Week 4 — Quality Baseline:** Create initial golden test set (20-50 cases), define manual evaluation process, document known limitations.

<Callout type="info">
**OCI Services:** Logging, Monitoring, Identity Domains
</Callout>
</DecisionBox>

**Key Deliverables:**
- Prompt repository structure
- OCI Logging enabled
- Basic dashboard with error rates
- 3 incident runbooks
- Golden test set with 50 cases

### Stage 2 → Stage 3: Measurement (Weeks 5-12)

<DecisionBox title="Stage 3 Implementation (Weeks 5-12)">

**Weeks 5-6 — Full Observability:** Instrument with OpenTelemetry, deploy APM for distributed tracing, create SLO dashboards, implement log correlation.

**Weeks 7-8 — Evaluation Pipeline:** Expand golden set to 200+ cases, integrate evaluation into CI/CD, implement semantic similarity metrics, add latency/cost assertions.

**Weeks 9-10 — Cost Management:** Tag all resources for cost allocation, implement per-request cost tracking, create cost dashboards by tenant/use-case, set up budget alerts.

**Weeks 11-12 — Governance:** Define prompt review process, implement approval workflows, create audit trail for changes, document compliance requirements.

<Callout type="info">
**OCI Services:** APM, Functions, DevOps Pipelines, Cost Analysis
</Callout>
</DecisionBox>

### Stage 3 → Stage 4: Optimization (Months 4-6)

| Capability | Implementation | OCI Service |
|------------|----------------|-------------|
| A/B Testing | Traffic splitting for prompt variants | OCI API Gateway + Feature Flags |
| Dynamic Routing | Route to optimal model based on query | OCI Functions + Generative AI |
| Automated Quality Gates | Block deployments below quality threshold | OCI DevOps + Functions |
| Anomaly Detection | ML-based detection of quality degradation | OCI Anomaly Detection |
| Cost Optimization | Automatic model downgrade under budget pressure | OCI Functions + Budgets |

### Stage 4 → Stage 5: Autonomy (Month 6+)

<DecisionBox title="Autonomous Operations Architecture">

Three capabilities feed into a unified **AI Control Plane**:

<ComparisonGrid>
  <ComparisonCard title="Self-Healing" color="cyan">
    Auto-rollback on quality drops. Circuit breakers for failed dependencies. Automatic failover to backup models or endpoints.
  </ComparisonCard>
  <ComparisonCard title="Self-Optimizing" color="emerald">
    Dynamic model selection per query. Cost/quality balancing based on budget and SLOs. Automatic scaling under load.
  </ComparisonCard>
  <ComparisonCard title="Self-Learning" color="violet">
    Continuous fine-tuning from feedback. Feedback loop integration (thumbs up/down). Automatic golden set expansion from production data.
  </ComparisonCard>
</ComparisonGrid>

<Callout type="info">
**Human Role at Stage 5:** Strategic decisions, exception handling, and capability design. The system operates autonomously within defined boundaries.
</Callout>
</DecisionBox>

---

## Observability Architecture

### The Four Pillars of AI Observability

<ArchitectureImage
  src="/images/blog/oci-series/observability-4-pillars.png"
  alt="4 Pillars of AI Observability — Traces (what happened?), Metrics (how is it performing?), Logs (what did it decide?), and Evals (is it working well?) — all converging into a Unified Observability Console."
  caption="Traditional observability covers traces, metrics, and logs. AI systems add a fourth pillar: Evals — continuous quality measurement that answers 'is the model still working well?'"
/>

### Distributed Trace Structure

<DecisionBox title="Agent Trace Anatomy">

**Trace ID:** `abc123` | **Duration:** 4.2s | **Status:** Success

1. **SPAN: `api_request`** (800ms) — `POST /api/agent/query` for `tenant-xyz`
2. **SPAN: `agent_orchestration`** (3200ms) — `sales-assistant v2.1.0`
   - **SPAN: `llm_call_planning`** (400ms) — Cohere Command R+, 850→120 tokens, $0.0034
   - **SPAN: `tool_execution`** (1800ms) — `search_knowledge_base("Q3 revenue by region")`
     - **SPAN: `vector_search`** (600ms) — Autonomous DB 23ai, 5 results, min similarity 0.82
     - **SPAN: `rerank`** (200ms) — Cohere Rerank v3
   - **SPAN: `llm_call_synthesis`** (1000ms) — Cohere Command R+, 2100→450 tokens, $0.0089, groundedness: 0.94
3. **SPAN: `response_streaming`** (200ms)

**Total Cost:** $0.0123 | **Quality:** groundedness=0.94, relevance=0.91

<Callout type="info">
Every LLM call, tool execution, and database query becomes a span. This lets you trace cost, latency, and quality to the exact step that caused an issue.
</Callout>
</DecisionBox>

### SLO Definition Framework

| SLO Category | Metric | Target | Burn Rate Alert | OCI Implementation |
|--------------|--------|--------|-----------------|-------------------|
| **Availability** | Success rate | 99.9% | 1% in 1hr | OCI Monitoring Alarms |
| **Latency** | P95 response time | < 5s | P95 > 8s | APM Service Level Objectives |
| **Quality** | Groundedness score | > 90% | < 85% avg | Custom metrics + Functions |
| **Cost** | Cost per query | < $0.05 | > $0.10 avg | Cost Analysis + Budgets |
| **Safety** | Harmful output rate | < 0.1% | Any detection | Logging + Content Moderation |

### OCI Observability Stack

<DecisionBox title="OCI Observability Stack — 5 Layers">

**Layer 1 — Application:** Your AI application instrumented with OpenTelemetry SDK.

**Layer 2 — Collection:** APM Agent (traces), Logging Agent (logs), Connector Hub (metrics) — three parallel pipelines.

**Layer 3 — Storage:** APM Trace Storage, Logging Service, Monitoring Metrics — each pipeline stores to its native service.

**Layer 4 — Analysis:** OCI Logging Analytics — log correlation with traces, ML-powered anomaly detection, custom dashboards, saved searches.

**Layer 5 — Alerting:** OCI Alarms (threshold-based), Notifications (PagerDuty, Slack, email), Events Service (automation triggers).

<Callout type="info">
**Key insight:** The three data types (traces, logs, metrics) flow through separate collection and storage pipelines, but converge in the analysis layer via Logging Analytics. This lets you correlate a latency spike (metric) with the specific trace and log entries that caused it.
</Callout>
</DecisionBox>

---

## Evaluation Pipeline Architecture

### Offline vs Online Evaluation

<DecisionBox title="Evaluation Architecture">

<ComparisonGrid>
  <ComparisonCard title="Offline Evaluation (Pre-deployment)" color="cyan">
    **Flow:** Golden Dataset (500+ cases) → Agent Under Test → Evaluator Functions (semantic, factual, safety) → Quality Gate (pass ≥95%, fail &lt;95%)

    **Outcome:** Pass → Deploy. Fail → Block deployment. No bad prompts reach production.
  </ComparisonCard>
  <ComparisonCard title="Online Evaluation (Post-deployment)" color="emerald">
    **Flow:** Live Traffic → Production Agent → Sampling Evaluator (5% of requests) → Dashboard + SLO Alerts

    **Feedback loop:** User feedback (thumbs up/down, corrections) feeds back into the golden set for continuous improvement.
  </ComparisonCard>
</ComparisonGrid>

<Callout type="warning">
**Both are required.** Offline evaluation catches regressions before deployment. Online evaluation catches real-world drift that test sets miss. Running only one leaves a blind spot.
</Callout>
</DecisionBox>

### Evaluation Metrics Framework

| Category | Metric | Measurement Method | Target | OCI Implementation |
|----------|--------|-------------------|--------|-------------------|
| **Semantic** | Response relevance | Embedding similarity to expected | > 0.85 | Functions + Generative AI |
| **Factual** | Groundedness | Citation verification | > 0.90 | Functions + custom |
| **Completeness** | Coverage | Checklist evaluation | > 0.80 | LLM-as-judge |
| **Safety** | Harmful content | Classifier detection | < 0.001 | Content Moderation API |
| **Latency** | Response time | P95 measurement | < 5s | APM |
| **Cost** | Per-request cost | Token tracking | < $0.05 | Custom metrics |

### Golden Set Design

<DecisionBox title="Golden Set Architecture">

**Repository structure:**

```
test_cases/
├── category_customer_service/
│   ├── refund_requests.yaml        (50 cases)
│   ├── product_inquiries.yaml      (50 cases)
│   └── escalation_scenarios.yaml   (30 cases)
├── category_technical_support/
│   ├── troubleshooting.yaml        (40 cases)
│   └── how_to_guides.yaml          (30 cases)
└── category_edge_cases/
    ├── adversarial_inputs.yaml     (50 cases)
    ├── out_of_scope.yaml           (30 cases)
    └── language_variations.yaml    (20 cases)
evaluators/
├── semantic_similarity.py
├── groundedness_check.py
└── safety_classifier.py
```

**Test case schema (YAML):**

```yaml
id: "cs-refund-001"
category: "customer_service"
input: "I want to return my order from last week"
context:
  order_id: "ORD-12345"
  order_date: "2026-01-14"
  status: "delivered"
expected:
  tool_calls: ["get_order", "initiate_return"]
  must_contain: ["return label", "refund timeline"]
  must_not_contain: ["cannot", "unable"]
constraints:
  max_latency_ms: 5000
  max_tool_calls: 3
```

**Coverage requirements:** Minimum 500 total cases, 20% adversarial/edge cases, 10% out-of-scope queries. Updated monthly with production failures.
</DecisionBox>

### CI/CD Integration

<DecisionBox title="Evaluation in CI/CD Pipeline">

**Pipeline flow:** Developer pushes prompt change →

1. **Lint Check** — Validate prompt syntax and structure
2. **Unit Tests** — Test individual components
3. **Golden Set Eval** → **Quality Gate** (pass ≥95%, latency OK, cost OK)
4. **Staging Deploy** → **Smoke Tests** (key flows working)
5. **Production Deploy** (canary at 5%)

```yaml
# OCI DevOps Pipeline
stages:
  - stage: evaluate
    jobs:
      - job: golden_set
        steps:
          - run: python -m eval.golden_set
                   --min-pass-rate 0.95
                   --max-avg-latency 3000
                   --max-avg-cost 0.05
          - publish: eval-results.json
        failOn: quality_gate_failed
```

<Callout type="warning">
**The quality gate is the critical addition.** Without it, a prompt change that degrades quality by 10% will silently ship to production. The gate enforces minimum pass rate, maximum latency, and maximum cost per request.
</Callout>
</DecisionBox>

---

## Prompt & Agent Lifecycle Management

### Prompts as Code

<DecisionBox title="Prompt Repository Structure">

```
prompts/
├── agents/
│   ├── sales-assistant/
│   │   ├── v1.0.0/
│   │   │   ├── system.md          # System prompt
│   │   │   ├── tools.yaml         # Tool definitions
│   │   │   ├── examples.json      # Few-shot examples
│   │   │   ├── guardrails.yaml    # Safety rules
│   │   │   └── metadata.yaml      # Version info
│   │   ├── v1.1.0/
│   │   └── CHANGELOG.md
│   └── support-agent/
├── components/                     # Reusable across agents
│   ├── personality/
│   │   ├── professional.md
│   │   └── friendly.md
│   └── guardrails/
│       ├── pii-protection.yaml
│       └── competitor-mentions.yaml
├── registry.yaml                   # Agent registry
└── templates/                      # Agent templates
```

```yaml
# registry.yaml — tracks which version is deployed where
agents:
  sales-assistant:
    production: v1.1.0
    staging: v1.2.0-beta
    deprecated: [v0.9.0]
    model: cohere.command-r-plus
    owner: sales-team
    review_required: true
```

<Callout type="info">
**Semantic versioning for prompts:** Major version = behavior change. Minor = improvement. Patch = fix. This lets you roll back to any prior version instantly.
</Callout>
</DecisionBox>

### Deployment Strategies

<DecisionBox title="Deployment Strategy Comparison">

<ComparisonGrid>
  <ComparisonCard title="Blue/Green" color="cyan">
    **How it works:** Run two identical environments. Route 100% traffic to Blue (current). Deploy to Green (new). Switch all traffic instantly.

    **Rollback:** Instant — switch back to Blue. **Resource cost:** 2x during deploy. **Best for:** Major changes, breaking changes, instant rollback needed.
  </ComparisonCard>
  <ComparisonCard title="Canary" color="emerald">
    **How it works:** Deploy new version alongside current. Route 5% traffic to canary. Gradually increase (5% → 30% → 100%) if metrics hold.

    **Rollback:** Fast — route back to 0%. **Resource cost:** 1.05x. **Best for:** Incremental improvements, A/B testing, prompt optimizations.
  </ComparisonCard>
  <ComparisonCard title="Shadow" color="violet">
    **How it works:** Send every request to both production (serves response) and shadow (logs only, response discarded). Compare outputs offline.

    **Rollback:** N/A — shadow never serves users. **Resource cost:** 2x always. **Best for:** Testing new models with zero user impact.
  </ComparisonCard>
</ComparisonGrid>
</DecisionBox>

### Deployment Decision Framework

| Factor | Blue/Green | Canary | Shadow |
|--------|-----------|--------|--------|
| **Risk tolerance** | Low | Very low | None |
| **Rollback time** | Instant | Fast | N/A |
| **Resource cost** | 2x during deploy | 1.05x | 2x always |
| **Comparison data** | Before/after | Statistical | Side-by-side |
| **User impact on failure** | None (if quick) | Minimal (5%) | None |
| **Best use case** | Breaking changes | Optimizations | Model swaps |

---

## Cost Management Architecture

### Cost Attribution Model

<DecisionBox title="Cost Attribution Hierarchy">

| Level | Example | Monthly Cost |
|-------|---------|-------------|
| **Organization** | Total AI spend | $50,000 |
| **Business Unit** | Sales / Support / Ops | $20K / $15K / $15K |
| **Agent** | Sales Agent, Lead Scoring, Support Agent, Chat Bot, SRE Agent | $12K, $8K, $10K, $5K, $15K |

**Attribution tags** (applied to every request):
- `business_unit`: sales, support, operations
- `agent`: sales-assistant, support-agent, sre-agent
- `tenant`: customer-xyz, internal
- `environment`: prod, staging, dev

<Callout type="info">
Tag every request at the API Gateway level. This enables cost allocation dashboards that show exactly which team, agent, and customer is driving spend — essential for chargeback models.
</Callout>
</DecisionBox>

### Cost Components Breakdown

| Component | Typical Share | Optimization Strategy | OCI Control |
|-----------|--------------|----------------------|-------------|
| **LLM Inference** | 40-60% | Prompt optimization, caching, model selection | Generative AI pricing tiers |
| **Embedding** | 10-20% | Batch processing, caching | Generative AI embed pricing |
| **Vector DB** | 10-15% | Index optimization, partitioning | Autonomous DB tiers |
| **Compute** | 10-20% | Right-sizing, autoscaling | OKE node pools, Functions |
| **Storage** | 5-10% | Tiering, lifecycle policies | Object Storage tiers |
| **Network** | 5-10% | Regional deployment | VCN optimization |

### Cost Control Mechanisms

<DecisionBox title="Cost Control Hierarchy — 4 Layers">

**Layer 1 — Per-Request Limits:** Reject if `estimated_cost > $0.50` or `token_count > 100,000`. Prevents runaway single requests.

**Layer 2 — Per-User Limits:** Track `user_daily_cost`. Throttle user if daily spend exceeds $10. Notify user of limit reached.

**Layer 3 — Per-Tenant Budget:** Track `tenant_monthly_cost`. Alert at 80% of budget. At 100%, downgrade to cheaper model or block requests.

**Layer 4 — Organization Hard Stop:** If `org_monthly_cost > hard_limit`, trigger emergency shutdown and page on-call. This is the last line of defense.

```python
# Layer 3 example
if tenant_monthly_cost > tenant.budget * 0.8:
    alert("80% of budget consumed")
if tenant_monthly_cost > tenant.budget:
    downgrade_model(tenant_id)  # Use cheaper model
```

<Callout type="info">
**OCI Implementation:** Budgets for alerts, Functions for enforcement logic, API Gateway for rate limiting, Events + Notifications for alerting.
</Callout>
</DecisionBox>

### Cost Optimization Strategies

| Strategy | Effort | Savings | Implementation |
|----------|--------|---------|----------------|
| **Prompt caching** | Medium | 20-40% | OCI Functions + Redis |
| **Model tiering** | Low | 30-50% | Route simple queries to smaller models |
| **Batch embedding** | Low | 10-20% | Queue and batch embedding requests |
| **Response caching** | Medium | 15-30% | Cache common queries |
| **Token optimization** | High | 10-20% | Prompt engineering |
| **Semantic caching** | High | 20-40% | Cache similar queries |

---

## Incident Response Framework

### Incident Classification Matrix

<DecisionBox title="Incident Severity Matrix">

| Frequency / Impact | Low Impact | High Impact | Critical |
|---|---|---|---|
| **High Frequency** | **P3** — Occasional incorrect responses. Response: 4hr, Resolve: 24hr | **P2** — Widespread quality degradation. Response: 1hr, Resolve: 8hr | **P1** — Service down or data breach. Response: 15min, Resolve: 4hr |
| **Low Frequency** | **P4** — Edge case failures. Response: 24hr, Resolve: 72hr | **P3** — Specific use case broken. Response: 4hr, Resolve: 24hr | **P2** — Targeted attack or compliance issue. Response: 1hr, Resolve: 8hr |

<Callout type="warning">
**AI-specific twist:** Quality degradation (P2) can be harder to detect than a full outage (P1). The system is "up" but returning subtly wrong answers. This is why continuous evaluation matters more than uptime monitoring alone.
</Callout>
</DecisionBox>

### Incident Response Runbooks

<DecisionBox title="Runbook: Model Quality Degradation">

**Trigger:** Groundedness score drops below 85% for 15+ minutes | **Severity:** P2 (High)

**Step 1 — Confirm (5 min):** Check Logging Analytics for error patterns. Review recent deployments. Check GenAI service status. Sample 5 recent requests manually.

**Step 2 — Root Cause (10 min):**
- Recent prompt change? → Check prompt registry
- Knowledge base update? → Check vector DB logs
- Model endpoint issue? → Check GenAI metrics
- Upstream data change? → Check integration logs

**Step 3 — Mitigate (15 min):**
- **If prompt change** → Roll back to previous version via blue/green switch
- **If model issue** → Switch to fallback model, enable degraded mode
- **If knowledge base** → Disable RAG (model-only responses), roll back KB to previous snapshot

**Step 4 — Communicate:** Update incident channel, notify stakeholders if customer-facing, update status page.

**Step 5 — Resolve:** Confirm metrics back to normal, document root cause, schedule post-mortem, create follow-up tickets for prevention.
</DecisionBox>

### Common Incident Types

| Incident Type | Detection | Immediate Action | OCI Tools |
|---------------|-----------|------------------|-----------|
| **Model timeout** | APM latency spike | Switch to fallback model | Generative AI + Functions |
| **Tool failure** | Error rate increase | Circuit breaker + fallback | Functions + Logging |
| **Quality degradation** | Eval score drop | Roll back prompt | DevOps + Monitoring |
| **Cost overrun** | Budget alert | Rate limit + downgrade | Budgets + API Gateway |
| **Prompt injection** | Pattern detection | Block + investigate | Logging Analytics |
| **Data breach** | Audit anomaly | Isolate + investigate | Security Zones + Audit |

### Automated Remediation

<DecisionBox title="Auto-Remediation Architecture">

**Flow:** OCI Alarms (latency, error rate, quality) → OCI Events (trigger) → OCI Functions (evaluate severity, execute playbook) → **Auto Actions** or **Human Review**

<ComparisonGrid>
  <ComparisonCard title="Auto Actions" color="emerald">
    Rollback prompt version. Scale up compute. Failover to backup model. Enable rate limiting. Triggered automatically for known, well-defined failure modes.
  </ComparisonCard>
  <ComparisonCard title="Human Review" color="amber">
    P1/P2 incidents. Novel issues not matching known patterns. Escalated automatically with full context (traces, logs, metrics) attached.
  </ComparisonCard>
</ComparisonGrid>

**Remediation rules:**

| Condition | Duration | Action |
|-----------|----------|--------|
| `latency_p95 > 10s` | 5 min | `scale_up()` |
| `error_rate > 5%` | 3 min | `enable_circuit_breaker()` |
| `quality_score < 80%` | 10 min | `rollback_prompt()` |
| `cost_rate > 2x baseline` | 1 hr | `enable_rate_limit()` |
</DecisionBox>

---

## Security and Compliance

### Security Control Framework

<DecisionBox title="AI Security Control Layers">

**Layer 1 — Input Protection:** Prompt injection detection (pattern matching + ML), input sanitization, length limits, rate limiting per user/tenant. *OCI: API Gateway + WAF + Functions.*

**Layer 2 — Processing Protection:** Tool execution sandboxing, least-privilege data access, context isolation between tenants, timeout enforcement. *OCI: Container Security + Identity Domains + Functions.*

**Layer 3 — Output Protection:** PII detection and redaction, harmful content filtering, citation verification, competitor mention filtering. *OCI: Functions + Content Moderation + Logging.*

**Layer 4 — Audit and Compliance:** Full request/response logging (with PII redaction), model decision audit trail, access control audit, compliance reporting. *OCI: Logging + Audit + Data Safe + Compliance Reports.*

<Callout type="warning">
**Defense in depth:** Each layer catches what the previous one missed. Input protection blocks prompt injection. Processing protection limits blast radius if injection succeeds. Output protection catches leaked data. Audit ensures you can prove compliance after the fact.
</Callout>
</DecisionBox>

### Data Classification and Handling

| Classification | Description | Allowed Models | Logging | OCI Controls |
|----------------|-------------|----------------|---------|--------------|
| **Public** | Non-sensitive, shareable | All | Full | Standard |
| **Internal** | Business-sensitive | OCI-hosted only | Metadata | Private endpoints |
| **Confidential** | Customer data, PII | Dedicated cluster | Audit only | Customer keys |
| **Restricted** | Regulated data | Private Agent Factory | Audit only | Data residency |

### Compliance Mapping

| Requirement | Control | OCI Implementation |
|-------------|---------|-------------------|
| **GDPR Art. 17** | Data deletion | Object Storage lifecycle + DB purge |
| **GDPR Art. 22** | Explainability | Trace logging + decision audit |
| **SOC 2 CC6.1** | Access control | Identity Domains + Policies |
| **HIPAA** | PHI protection | Data Safe + Encryption + Audit |
| **PCI DSS** | Cardholder data | Tokenization + Isolation |

---

## Team Operating Model

### Roles and Responsibilities

<DecisionBox title="AI Platform Team Structure">

**AI Platform Lead** → Strategy, standards, escalation. Reports to three specialist roles:

<ComparisonGrid>
  <ComparisonCard title="ML Engineer" color="cyan">
    Model ops, evaluation pipelines, fine-tuning. Owns model selection and quality gates.
  </ComparisonCard>
  <ComparisonCard title="Platform Engineer" color="emerald">
    Infrastructure, observability, security. Owns deployment pipelines and monitoring.
  </ComparisonCard>
  <ComparisonCard title="Prompt Engineer" color="violet">
    Agent design, quality testing, prompt optimization. Owns prompt versioning and evaluation.
  </ComparisonCard>
</ComparisonGrid>

**RACI Matrix** (R=Responsible, A=Accountable, C=Consulted, I=Informed):

| Activity | Lead | ML Eng | Platform | Prompt Eng |
|----------|------|--------|----------|------------|
| Model selection | A | R | C | C |
| Prompt design | A | C | I | R |
| Infrastructure | A | C | R | I |
| Quality gates | A | R | C | R |
| Incident response | A | R | R | C |
| Cost management | R | C | R | I |
</DecisionBox>

### On-Call Structure

| Rotation | Scope | Hours | Escalation |
|----------|-------|-------|------------|
| **Primary** | All P1/P2 incidents | 24/7 | ML Engineer or Platform Eng |
| **Secondary** | Escalation backup | 24/7 | AI Platform Lead |
| **Specialist** | Model-specific issues | Business hours | ML Engineer (on-call) |

### Communication Channels

| Channel | Purpose | Response Time |
|---------|---------|---------------|
| **#ai-platform-alerts** | Automated alerts | Monitor during on-call |
| **#ai-platform-support** | Team support requests | 4 hours (business) |
| **#ai-platform-incidents** | Active incident coordination | Immediate |
| **Weekly AI Ops Review** | Metrics review, improvements | Weekly |

---

## Production Readiness Checklist

### Pre-Launch Checklist

| Category | Item | Required | Notes |
|----------|------|----------|-------|
| **Security** | RBAC configured | Yes | Identity Domains |
| **Security** | Network isolation | Yes | VCN + Private endpoints |
| **Security** | Secrets management | Yes | OCI Vault |
| **Security** | Input validation | Yes | Functions + WAF |
| **Reliability** | Timeouts configured | Yes | All external calls |
| **Reliability** | Retries with backoff | Yes | Transient failures |
| **Reliability** | Fallback behavior | Yes | Graceful degradation |
| **Reliability** | Circuit breakers | Recommended | High-frequency tools |
| **Quality** | Golden set tests | Yes | 500+ cases |
| **Quality** | CI/CD integration | Yes | DevOps pipelines |
| **Quality** | Online evaluation | Recommended | Sampling |
| **Cost** | Budget alerts | Yes | OCI Budgets |
| **Cost** | Rate limiting | Yes | API Gateway |
| **Cost** | Cost attribution | Recommended | Tagging |
| **Observability** | Distributed tracing | Yes | APM + OpenTelemetry |
| **Observability** | SLOs defined | Yes | Monitoring |
| **Observability** | Alerting configured | Yes | Notifications |
| **Governance** | Prompt versioning | Yes | Git repository |
| **Governance** | Audit logging | Yes | Logging + Audit |
| **Governance** | Runbooks documented | Yes | At least 3 |

### Launch Day Checklist

- [ ] All pre-launch items verified
- [ ] Staging environment tested
- [ ] Rollback procedure documented and tested
- [ ] On-call schedule confirmed
- [ ] Stakeholders notified
- [ ] Monitoring dashboards visible
- [ ] Canary deployment started (5%)
- [ ] Initial metrics baseline captured

---

## Series Complete

This three-part series has provided a comprehensive framework for production AI systems on OCI:

1. **[Part 1: Architecture](/blog/production-llm-agents-oci-part-1-architecture)** — Six-plane enterprise architecture with OCI service mapping
2. **[Part 2: Agent Patterns](/blog/production-llm-agents-oci-part-2-agent-patterns)** — Six orchestration patterns with decision frameworks
3. **Part 3: Operating Model** — 5-stage maturity model, evaluation pipelines, incident response, and team structure

### Next Steps

| Maturity Stage | Recommended Actions |
|----------------|---------------------|
| **Stage 1** | Start with [Part 1](/blog/production-llm-agents-oci-part-1-architecture) architecture patterns |
| **Stage 2** | Implement basic monitoring and prompt versioning |
| **Stage 3** | Build evaluation pipelines using this guide |
| **Stage 4** | Implement A/B testing and automated quality gates |
| **Stage 5** | Design autonomous operations architecture |

---

## Resources

- [OCI AI Agent Platform Documentation](https://docs.oracle.com/en-us/iaas/Content/ai-agents/home.htm)
- [OCI APM for GenAI Observability](https://blogs.oracle.com/cloud-infrastructure/post/oci-apm-rag-solutions)
- [OCI DevOps CI/CD](https://docs.oracle.com/en-us/iaas/Content/devops/using/home.htm)
- [OCI Budgets and Cost Management](https://docs.oracle.com/en-us/iaas/Content/Billing/Concepts/budgetsoverview.htm)
- [OpenTelemetry OCI Integration](https://docs.oracle.com/en-us/iaas/application-performance-monitoring/)
- [OCI AI Blueprints (GitHub)](https://github.com/oracle-quickstart/oci-ai-blueprints)

---

*Visit the [AI CoE Hub](/ai-architect/ai-coe-hub) for all resources, decision frameworks, and links to Oracle Architecture Center patterns.*
